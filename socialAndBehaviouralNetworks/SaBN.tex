\documentclass[]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}

\title{Social and Behavioural Networks}
\author{}

\begin{document}

\maketitle
\tableofcontents

\clearpage

\section{First-Edge algorithm}
Let $G = (V, E)$ be an undirected graph. We want to infer the structure of $G$ by looking at how information spreads through it.\\We consider information flow to follow a push-pull model. In this model information randomly appears at a source node $s$, at each round the nodes that have the information push it to an uninformed neighbouring node, chosen \textbf{U}niformly \textbf{A}t \textbf{R}andom, the nodes that do not have the information try to pull it from a neighbouring node, chosen UAR.\\If we can track when information appears at each node we can build a trace, that is, an ordered list of pairs $p_i = (v_i, T_i)$ where $T_i \in \mathbb{R}^+$ is the time at which the information first appeared at node $v_i \in V$ and $\forall i,j \, |\, i\neq j  \quad p_i \geq p_j \implies T_i \geq T_j$.

\begin{algorithm}[H]
	\caption{First-Edge( set of traces: Traces)}
	\label{alg:first-edge}
	\begin{algorithmic}
		\STATE $E \leftarrow \emptyset$
		\FOR{trace in Traces}
			\STATE $E\, \cup\!= \{trace[0],\, trace[1]\}$
		\ENDFOR
		\RETURN{$E$}
	\end{algorithmic}
\end{algorithm}

\newtheorem*{FirstEdgeBound}{First-Edge bound}
\begin{FirstEdgeBound}
	If $t\geq3n\Delta ln(n)$ (with $\Delta \leq n-1$ being the max degree of the graph) then the First-Edge algorithm is correct with probability $\geq 1-\frac{1}{n}$
\end{FirstEdgeBound}

\begin{proof}
	We can compute the probability that an edge $e = \{u, v\}$ will be the first to appear in a trace. Since it's a uniformly distributed random variable: \[\Pr\left\{\text{u will be the first node}\right\} = \frac{1}{n}\] Now we would like to know the probability 
	\[\Pr\{\underbrace{\text{v will be the second node }}_{v_{second}} | \underbrace{\text{ u is the first node} }_{u_{first}}\}\] 
	But since we don't know the structure of the graph we can only provide a lower bound:
	\begin{align*}
		\Pr&\{v_{second} |\ u_{first} \} = \frac{1}{n} \frac{1}{deg(u)} \geq \frac{1}{n\Delta}\\
		\Pr&\{\text{At least one of our } T \text{ traces begin with }u, v\}\\
		&= 1 - \Pr\{\text{None of our traces begin with }u, v\}\\
		&= 1- \left(1-\frac{1}{n\Delta}\right)t\\
	\end{align*}
	
	
Let's go back to our original statement:
\[	\Pr\{\text{A random trace does not begin with }u, v\} \leq 1 -\frac{1}{n\Delta}	\]
	
	\begingroup
	\addtolength{\jot}{1em}
	\begin{align*}
		\left(1 -\frac{1}{n\Delta}\right)^t &\leq \left(1 -\frac{1}{n\Delta}\right)^{3n\Delta ln(n)}\\
		&\leq \left(e^{-\frac{1}{n\Delta}}\right)^{3n\Delta ln(n)} = e^{-3nln(n)} = \frac{1}{n^3}
	\end{align*}
	\endgroup
	
\paragraph{Union bound lemma}
The probability of the union is $\leq$ the probability of the sum of the events:
\begin{align*}
\Pr\left(\bigcup_{e=1}^{t} \xi_e\right) \leq \sum_{e_i}^{t}\Pr\{\xi_e\}\\
\end{align*}	

Thus we have that:
\begin{align*}
	&\forall \{a,b\} \in E(G)\ \xi_{a,b} = (\text{none of our traces begin with a,b})\\
	&\Pr\{\xi_{a,b}\} \leq \frac{1}{n^3} \text{ Thus:}\\
	&\Pr\{\text{Our algorithm fails}\} =\Pr\{\bigcup_{\{a,b\}\in E(G)}^{} \xi_{a,b}\}\leq \sum\Pr\{\xi_{a,b}\} \leq n^2 \frac{1}{n^3} = \frac{1}{n}
\end{align*}

We can conclude the probability that our algorithm is correct is $1-\frac{1}{n}$. However if we increase the traces by a factor of $c$ we actually get\\ $\Pr\{failure\} = n^{2-3c}$. This algorithm thus gets pretty close to optimal as soon as the number of traces gets big enough. It can be shown that $\frac{n\Delta}{{ln}^2n}$ traces are not enough. Using all the trace one can reconstruct the graph with $O(\Delta^9ln(n))$ traces.
\end{proof}

\paragraph{Recap on random variables}
\begin{align*}
\text{Let } X = \sum_{i=0}^{n}x_i \text{ with }x_i \text{ random variables}\\
\mathbb{E}[X] = \sum_{i=0}^{n} \mathbb{E}[x_i] \text{ by linearity of the expected value}\\
\end{align*}

E.g.:
\begin{align*}
	&\forall i \in [n] \text{ flip a fair coin }c_i \text{ we have}\\
	&x_i = \begin{cases}
		1 \iff c_i \text{ is heads}\\
		0 \text{ otherwise}
	\end{cases}\\
	&\Pr( X = k) = {n \choose k}2^{-n}\\
\end{align*}

If our coins are not fair we have of course:
\begin{align*}
&\Pr\{c_i = \text{Heads}\} = p\\
&\Pr\{X = k \} = {n \choose k}p^{k}(1-p)^{n-k}\\
\end{align*}

\paragraph{Chernoff Bound}
If $\{x_i\}_{i=0}^{n}$ are all mutually independent variables and $0\leq x \leq 1 \forall i$ and we define $X = \sum_{i=1}^{n} x_i$ . We can define the following bounds:
\begin{align*}
&\Pr\{x > (1+\epsilon) \mathbb{E}[X]\} \leq \exp({-\epsilon}^{-2} \frac{\mathbb{E}[X]}{3}) \\
&\Pr\{x < (1+\epsilon) \mathbb{E}[X]\} \leq \exp({-\epsilon}^{-2} \frac{\mathbb{E}[X]}{3}) \\
\end{align*}
E.g.
\begin{align*}
&\forall i \in [n]\ \Pr\{x_i = 1\} = p  \implies \mathbb{E}[X] = pn\\
&\Pr\{X > (1+\epsilon) \mathbb{E}[X]\} \leq \exp\left({-\epsilon}^{-2} \frac{pn}{3}\right),\, X = \sum_{i=1}^{n} x_i \\
\end{align*}

We do not prove that, thanks goodness.

\paragraph{Chain letters} Chain letters are mails sent by a person which is then forwarded to other people who can add their in signature at the end of the letter.
\begin{align*}
&E = \text{\# exposed nodes} &\parbox{13em}{The nodes of the chain letter tree that have been exposed}\\
&L = \text{\# leaves in the tree}&\parbox{13em}{The leaves of the clt}\\
&N = \text{\# nodes in the tree}&\parbox{13em}{The nodes in the clt}\\
\end{align*}

We can guess the total number of nodes as \[\hat{N} = \frac{E}{\hat{\delta}} = \frac{E}{\left(\frac{E-L}{M-L}\right)} \]. Where $M$ is the number of nodes we suppose are in the graph. We now have a maximization problem.

\begin{align*}
&G = (V, E)\\
&\forall v \in V,\, S(v) = \text{Neighbourhood}(v)\\
&\mathscr{S} = \{S_v, \dots ,S_{v_n} \}\ \text{set of all close neighbourhood sets}\\
\end{align*}

Given $k\geq 1$ find $\mathscr{S}_k \subset \mathscr{S},\, |\mathscr{S}_k| = k$ such that $|\bigcup\limits_{\mathscr{S}\in \mathscr{S}_k} S|$ is maximized.

\begin{algorithm}[H]
	\caption{Greedy-Cover(neighbourhood set: $\mathscr{S}$, int: k) }
	\label{alg:greedy-cover}
	\begin{algorithmic}
		\STATE $X_0 \leftarrow \emptyset$
		\FOR{$i$ in range(k)}
			\STATE pick a $S \in \mathscr{S}$ s.t. $|X_{i-1} \cup S|$ is maximized
			\STATE $X_i = X_{i-1} \cup S$
		\ENDFOR
		\RETURN $X_k$
	\end{algorithmic}
\end{algorithm}

\newtheorem{greedycover}{Greedy Cover Bound}

\begin{greedycover}
	We call  $opt$ the value of the optimal solution,\\ $t_i = |x_i|$ the cardinality of the current solution at the step $i$. We call $g_i = opt - t_i$ the gain at step $i$. \\We can then state that: 
	\[
		t_k \geq \left(1-\frac{1}{e}\right)opt \iff g_k \leq \frac{1}{e}opt
	\]
	
	With $t_k$ the value returned by the algorithm.\\
	We will prove that the greedy cover algorithm returns a $\geq 1-\frac{1}{e}$ approximation of the problem.  $\left(1-\frac{1}{e} \approx 0.63\right)$. 
\end{greedycover}

\begin{proof}
	We define $n_i = |S_i \setminus X_{i-1}|$.
	\paragraph{Lemma 1} $n_{i+1} \geq \frac{g_i}{k}$ The elements that have 
	\begin{align*}
	&T \subset O^* &T_i = O^* \setminus X_i\\
	&S_1^*, \dots ,S_k^*&\parbox{15em}{sets chosen by the optimal solution}\\
	&\text{We always have that }&T\subseteq \bigcup_{i=1}^{k}S_i^*\\
	&\exists\ i\ \text{s.t. }|S_i^* \cap T| \geq \frac{|T|}{k}& \text{by contradiction } \forall\ i\ |S_i^* \cap T| \leq \frac{|T|}{k}\\
	\end{align*}
\end{proof}


\section{16-10-17}
Preferential attachment model: in this evolutionary model nodes get added one by one in the graph. The model starts with one node with an edge to itself. Whenever a node is added it will choose a neighbour in a ``rich-get-richer'' fashion. The probability that a node will be chosen as the neighbour for a new node is equal to \[\Pr\left\{\text{The new node will attach to }v_i\right\} = \frac{Deg(v_i)}{|E|}\]. We claim that the distribution of the degrees of the nodes will follow a power law.\\
\begin{proof}

Let's call ${{X}^{(t)}}_i = \text{\# nodes of degree } i \text{ in }G_t$. Then we have:
	\begin{align*}
		X_2^{(1)} &= 1\\
		X_t^{(1)} &= 0\, \forall i\neq 2
	\end{align*}
	
	Since:
	
		\begin{align*}
	|V(G_t) &= t\\
	|E(G_t) &= t\\
	\sum_{v \in V(G_t)}^{} d_v &= 2t = 2|E(G_t)|
			\end{align*}
			
It follows that:

		\begin{align*}
			\mathbb{E}\left[X_i^{(t-1)} \middle| \bar{X}^{(t-1)}\right] &=\left(X_i^{(t-1)} + 1\right)\Pr\left\{\parbox{16em}{Node t does not choose a node that used to have degree 1 as its neighbour}\right\} +\\
			&+ X_1^{(t)} \Pr\left\{\parbox{15em}{Node t chooses a deg-1 neighbour}\right\}\\
			&= \left(X_1^{(t-1)} + 1\right)\left(1-\frac{X_1^{(t-1)}}{2(t-1)}\right) + X_1^{(t-1)} \frac{X_1^{(t-1)}}{2(t-1)}\\
			&= X_1^{(t-1)} - \frac{X_1^{(t-1)}}{2(t-1)}) + X_1^{(t-1)} \frac{ X_1^{(t-1)} }{2(t-1)}+1
		\end{align*}
PLZ FEDERICO FIX DA EQUATION\\

Now if the equations above don't simplify we're in trouble since all we can say for certain is that: $\mathbb{E}[A+B] = \mathbb{E}[A] + \mathbb{E}[B]$ (linearity of the expected value) but not necessarily $\mathbb{E}[A^2+B^2] = \mathbb{E}[A]^2 + \mathbb{E}[B]^2$.\\


\begin{align*}
	\mathbb{E}\left[X_1^{(t)} \right] &= \sum_{c\geq0}^{}\left( \Pr\left\{ X_1^{(t-1)} = c \right\} \mathbb{E}\left[ X_1^{(t)} \middle| X_1^{(t-1)} = c \right] \right)\\
	&= \sum_{c\geq0}^{}\left( \Pr\left\{ X_1^{(t-1)} = c \right\}\left( \left( 1 - \frac{1}{2(t-1)} \right)c + 1 \right) \right)\\
	&= \sum_{c\geq0}^{}\left( \Pr\left\{ X_1^{(t-1)} = c \right\}\left( \left( 1 - \frac{1}{2(t-1)} \right)c\right) \right) + \underbrace{\sum_{c\geq0}^{}\left( \Pr\left\{ X_1^{(t-1)} = c \right\} \right)}_\text{1}\\
	&= \left( 1 - \frac{1}{2(t-1)}\right) \sum_{c\geq0}^{}\left( \Pr\left\{ X_1^{(t-1)} = c \right\}c \right) + 1\\
	&= \left( 1 - \frac{1}{2(t-1)}\right)+ 	\mathbb{E}\left[X_1^{(t-1)} \right] + 1\\
\end{align*}

\paragraph{Lemma 1}
\begin{align*}
\mathbb{E}\left[X_1^{(t)} \right] &= \left( 1 - \frac{1}{2(t-1)}\right)\mathbb{E}\left[X_1^{(t-1)} \right] + 1\\
\end{align*}

\paragraph{Lemma 2}
\begin{align*}
\mathbb{E}\left[X_i^{(t)} \right] &= \left( 1 - \frac{i}{2(t-1)}\right)\mathbb{E}\left[X_i^{(t-1)} \right] + \left(\frac{i-1}{2(t-1)}\right)\mathbb{E}\left[X_{i-1}^{(t-1)} \right]\\
\end{align*}

Let us consider $\forall i \geq 2$
\begingroup
\addtolength{\jot}{1em}
\begin{align*}
	\mathbb{E}\left[X_i^{(t-1)} \middle| \bar{X}^{(t-1)}\right] &= X_i^{(t-1)} + \Pr\left\{ X_i^{(t)} - X_i^{(t-1)} = 1\right\} - \Pr\left\{ X_i^{(t)} - X_i^{(t-1)} = -1\right\}\\
	&= X_i^{(t-1)} + \left( \frac{i-1}{2(t-1)}\right) X_{i-1}^{(t-1)} - \left(\frac{1}{2(t-1)}\right) X_{i}^{(t-1)}\\
	&= \left(1- \frac{i}{2(t-1)}\right) X_{i}^{(t-1)} + \left( \frac{i-1}{2(t-1)}\right) X_{i-1}^{(t-1)}\\
	&= \mathbb{E}\left[X_i^{(t)} \middle| X_i^{(t-1)},\,  X_{i-1}^{(t-1)}\right]
\end{align*}
\endgroup


\begingroup
\addtolength{\jot}{1em}
\begin{align*}
\mathbb{E}\left[X_i^{(t)} \right] &= \sum_{\substack{c \geq 0 \\ d \geq 0}}^{} \left( \Pr\left\{  X_i^{(t-1)} = c \wedge X_{i-1}^{(t-1)} = d \right\} \mathbb{E}\left[X_i^{(t)} \middle| X_i^{(t-1)} = c,\,  X_{i-1}^{(t-1)} = d\right] \right)\\
&= \sum_{\substack{c \geq 0 \\ d \geq 0}}^{} \left( \Pr\left\{  X_i^{(t-1)} = c \wedge X_{i-1}^{(t-1)} = d \right\} \left( \left(1- \frac{i}{2(t-1)}\right)c + \left( \frac{i-1}{2(t-1)}\right)d\right)\right)\\
&=  \sum_{\substack{c \geq 0 \\ d \geq 0}}^{} \left( \Pr\left\{  X_i^{(t-1)} = c \wedge X_{i-1}^{(t-1)} = d \right\} \left(1- \frac{i}{2(t-1)}\right)c\right)\\ &+  \sum_{\substack{c \geq 0 \\ d \geq 0}}^{} \left( \Pr\left\{  X_i^{(t-1)} = c \wedge X_{i-1}^{(t-1)} = d \right\}\left( \frac{i-1}{2(t-1)}\right)d \right)\\&=  \sum_{\substack{c \geq 0 \\ d \geq 0}}^{} \left( \Pr\left\{  X_i^{(t-1)} = c \right\} \left(1- \frac{i}{2(t-1)}\right)c\right)\\ &+  \sum_{\substack{c \geq 0 \\ d \geq 0}}^{} \left( \Pr\left\{ X_{i-1}^{(t-1)} = d \right\}\left( \frac{i-1}{2(t-1)}\right)d \right)\\
&= \left( 1 - \frac{i}{2(t-1)}\right)\mathbb{E}\left[X_i^{(t-1)} \right] + \left(\frac{i-1}{2(t-1)}\right)\mathbb{E}\left[X_{i-1}^{(t-1)} \right]\\
\end{align*}
\endgroup

We want to prove that $\mathbb{E}\left[X_i^{(t)} \right] \approx i^{-\alpha} t$. However, we will not actually attempt to come up with a function that satisfies our equations, we'll just be providing one and verify that it is indeed a solution. So we state that:
\[
	E_i = \frac{4}{i(i+1)(i+2)}
\]
is a solution for the recursive equations above. We can ``easily'' verify that:
\[
	E_i t - 1\leq \mathbb{E}\left[X_i^{(t)}\right] \leq E_i t + 1
\] 

by double induction, first on the degree $i$ of each node and then on the time step $t$ of our algorithm.

\begingroup
\addtolength{\jot}{1em}
\begin{align*}
\text{For }t = 1 \text{ we have:}&\ 0\leq \mathbb{E}\left[X_i^{(t)} \right] \leq 1\ \text{true}\\
\text{For }i = 1 \text{ we have:}&\ \mathbb{E}\left[X_1^{(t)} \right] =  \left( 1 - \frac{1}{2(t-1)}\right)\mathbb{E}\left[X_1^{(t-1)} \right] + 1\\
&\leq \left( 1 - \frac{1}{2(t-1)}\right)\left(\frac{4}{1\cdot2\cdot3}(t-1)+1\right) + 1\\
&= \left( 1 - \frac{1}{2(t-1)}\right)\left(\frac{2}{3}(t-1)+1\right) + 1\\
&= \frac{2}{3}(t-1) - \frac{2}{3}\frac{t-1}{2(t-1)} + 1 - \frac{1}{2(t-1)} + 1\\
&= \frac{2}{3}(t-1) + \left(1- \frac{1}{3}\right) + \left(1 - \frac{1}{2(t-1)}\right)\\
&= \frac{2}{3}t + \left(1 - \frac{1}{2(t-1)}\right)\\
&\leq \frac{2}{3}t  + 1\\
&= E_i t + 1\\
\end{align*}
\endgroup

Now that we're proved the base cases we can go for the general case:

\begingroup
\addtolength{\jot}{1em}
\begin{align*}
\mathbb{E}\left[X_i^{(t)} \right] &=  \left( 1 - \frac{i}{2(t-1)}\right)\mathbb{E}\left[X_i^{(t-1)} \right] + \left(\frac{i-1}{2(t-1)}\right)\mathbb{E}\left[X_{i-1}^{(t-1)} \right]\\
&\leq \left( 1 - \frac{i}{2(t-1)}\right)\left(E_i (t-1) + 1\right) + \left(\frac{i-1}{2(t-1)}\right)\left(E_{i-1} (t-1) + 1\right)\\
&= \left( 1 - \frac{i}{2(t-1)}\right)\left(\frac{4}{(i)(i+1)(i+2)} (t-1) + 1\right) + \left(\frac{i-1}{2(t-1)}\right)\left(\frac{4}{(i-1)(i)(i+1)} (t-1) + 1\right)\\ 
&= \frac{4(t-1)}{(i)(i+1)(i+2)} + 1 - \frac{2}{(i+1)(i+2)} - \frac{i}{2(t-1)} + \frac{2}{(i)(i+1)} + \frac{i-1}{2(t-1)}\\
&= \frac{4(t-1)}{(i)(i+1)(i+2)} + \frac{2}{i+1}\left(\frac{1}{i} - \frac{1}{i+2}\right) - \frac{i}{2(t-1)} \\
&= \frac{4(t-1)}{(i)(i+1)(i+2)} + \frac{2}{i+1}\left(\frac{i+2}{i(i+2)} - \frac{i}{i(i+2)}\right) - \frac{i}{2(t-1)}\\
&= \frac{4(t-1)}{(i)(i+1)(i+2)} + \frac{4}{(i)(i+1)(i+2)} - \frac{i}{2(t-1)}\\
&= \frac{4(t-1) - 4}{(i)(i+1)(i+2)} - \frac{i}{2(t-1)}\\
&= \frac{4(t-2)}{(i)(i+1)(i+2)} - \underbrace{\frac{i}{2(t-1)}}_{\geq 0} \leq \frac{4t}{(i)(i+1)(i+2)} + 1\\
\end{align*}
\endgroup
\end{proof}

\section{19-10-17}
The core of a graph is ???.%FIX PLS

\paragraph{Small world phenomenon}
In the 60s Milgram wanted to prove that for any pair of people A, B there existed a chain of acquaintances that connected A to B. He selected a 1000 random people in Boston and asked them to send a letter to an unknown person in NY by only being allowed to send the letter to a person whom they knew. Milgram found that the average length of the chains was $\leq 6$. Kleinberg wanted to find a model that could explain this phenomenon.\\
He supposed that each person lived in a point of a grid.
\[
\begin{Bmatrix}
\cdot & \cdot &\cdot &\cdot &\cdot\\
\cdot & \cdot &\cdot &\cdot &\cdot\\
\cdot & p_{i,j} &\cdot &\cdot &\cdot\\
\cdot & \cdot &\cdot &\cdot &\cdot\\
\cdot & \cdot &\cdot &\cdot &\cdot\\
\end{Bmatrix}\]

\begin{align*}
	V &=\{(i,j) | 1\leq i,j\leq n\}\\
\end{align*}
\begin{align*}
	\forall (i, j) \in V & ((i,j), ((i+1),j)) \in E\\
						 & ((i,j), (i,(j+1))) \in E\\
						 & ((i,j), (i,(j-1)) \in E\\
						 & ((i,j), ((i-1),j)) \in E\\
\end{align*}

\begin{align*}
\text{Each} (i,j) \in V \text{ has one long-range link}\\
\Pr\{(i,j)\rightsquigarrow (k,l)\} &\approx (|i-k|+|j-l|)^{-2}\\
&= l_m((i-k),(j-l))^{-2}
\end{align*}

We can express this as:
\[
\Pr\{u \rightsquigarrow v\} = \frac{d(u,v)^{-2}}{\sum_{z\neq u}^{}d(u,z)^{-2}}
\]

\begin{algorithm}[H]
	\caption{Geo-Greedy(node: v, node: t, message: M) }
	\label{alg:geo-greedy}
	\begin{algorithmic}
		\IF{$v \neq t$}
			\STATE $w\in Neighbours(v)$ that has the smallest $l_m$-distance to t (breaking ties arbitrarily)
			\STATE $v$ forwards (M, t) to w
		\ENDIF
	\end{algorithmic}
\end{algorithm}

\newtheorem{GeoGreedy}{Geo-Greedy Bound}
\begin{GeoGreedy}
	GeoGreedy will make and expected number of steps, to go from any s to t, of $O({log}^2n)$
\end{GeoGreedy}

\begin{proof}
		We call the nodes that are at distance $r$ from $u$:
	\begin{align*}
	S^{(r)}_u &= \{z | d(z,u) = r\}\\
	&\leq \{(i+k, j+r+k) | 0\leq k \leq r \} \cup\\ 
	&\leq \{(i+k, j-r+k) | 0\leq k \leq r \} \cup\\ 
	&\leq \{(i-k, j-r+k) | 0\leq k \leq r \} \cup\\ 
	&\leq \{(i-k, j+r+k) | 0\leq k \leq r \}\\ 
	|S^{(r)}_u| &= 4(r+1) - \underbrace{4}_{\text{double corners count}} = 4r
	\end{align*}
	
	\paragraph{Lemma 1} 
	\begin{align*}
	|S^{(r)}_u| &\leq 4r &\forall r \geq 1\\
	|S^{(r)}_u| &\geq r  &\forall r \leq \frac{n}{2}
	\end{align*}
	
	\begin{align*}
		\sum_{z\neq u}^{}d(u,z)^{-2} &=\\
		&= \sum_{i\geq 1}^{2n-2}\left( i^{-2} \cdot \overbrace{ | \{z | d(z, u) = i\}| }  \right)\\
		&= \sum_{i\geq 1}^{2n-2}\left( i^{-2} 4i \right) = 4\sum_{i\geq 1}^{2n-2}\left( \frac{1}{i}  \right)\\
		&\leq 4(1+\ln(2n-1)) \leq 4(1+\ln(2n)) = 4\ln(2en) < 4\ln(6n)
	\end{align*}
	
	\paragraph{Lemma 2} From that previous upper bound we can rewrite the original squiggly probability as:
	\[
	\Pr\{u \rightsquigarrow v\} = \frac{d(u,v)^{-2}}{4ln(6n)}
	\]
	
	Geo-greedy is in phase j if its current node c satisfies $2< d(c,t) \leq 2^{j+1}$. Let's define $x_i =$\# number of steps that we spend in phase j. We want to prove that:
	\[
		\text{\# of steps} = \sum_{j=0}^{1+\ln(n)} x_j  
	\] 
	
	\paragraph{Lemma 3} Let's define what a bowl is in lattice space:
	\begin{align*}
	B_u^{(r)} &= \{v | d(u,v) \leq r\},\ r\leq 2n - 2\\
	|B_u^{(r)}| &\geq |B_u^{(\lceil \frac{r}{4} \rceil)}| = \sum_{i=0}^{\lceil \frac{r}{4} \rceil} |S^{(i)}_u| = 1 + \sum_{i=1}^{\lceil \frac{r}{4} \rceil}i = 1 + \frac{\lceil \frac{r}{4} \rceil(\lceil \frac{r}{4} \rceil +1)}{2} \geq \frac{r^2}{32}
	\end{align*}
	
	Phase j
	\begin{align*}
 	x \in B_t^{(2^j)}\\
	d(c, x) &\leq d(c, t) + d(t, x) \\
	&\leq 2^{j+1} + d(t,x) \leq 2^{j+1} + 2^j \leq 2^{j+2}
\end{align*}
	\begin{align*}
	\Pr\{c \rightsquigarrow x\} \geq \frac{d(c,x)^{-2}}{4\ln(6n)} \geq \frac{2^{-2j-4-2}}{\ln(6n)} = \frac{2^{-2j-6}}{\ln(6n)}\\
	\Pr\{\exists x \in B_t^{(2^j)} c \rightsquigarrow x\} &= \sum_{ x \in B_t^{(2^j)}}^{}\Pr\{c \rightsquigarrow x\}\\
	&\geq \sum_{ x \in B_t^{(2^j)}}^{} \frac{2^{-2j-6}}{\ln(6n)} = \frac{(2^j)^2}{32} \frac{2^{-2j-6}}{\ln(6n)} \\
	&= \frac{2^{2j-5-2j-6}}{\ln(6n)} = \frac{2^{-11}}{\ln(6n)} = \frac{1}{2048\ln(6n)}
	\end{align*}
	
	We can now finally compute the expected value of the number of nodes in the chain to get from node s to t.
	
	\begin{align*}
		\mathbb{E}[x_j] = \sum_{i=1}^{\infty} \Pr \{x_j \geq i \} &\leq \sum_{i=1}^{\infty} \left(1- \frac{1}{2048\ln(6n)} \right)^{i-1}\\ 
		&= \sum_{i=0}^{\infty} \left(1- \frac{1}{2048\ln(6n)} \right)^{i} = 2048\ln(6n)\\
		&\textit{Since}\ \sum_{i=0}^{\infty} (1-x) = \frac{1}{x},\ 0<x<1
	\end{align*}
	
\end{proof}

\section{Densest subgraph}

\begin{align*}
&\text{let } G= (V,E) \text{be an undirected graph}\\
&\text{We want to find } S \subseteq V \text{ that maximizes } f(S) = \frac{|E(S)|}{|S|}\\
&E(S) = \{ \{i, j\} | \{i, j\} \in E, \{i, j\} \subseteq S \}
\end{align*}

That is we want to find the subgraph where the ratio of the number of edges to the number of nodes is highest. We're interested in this problem to find communities. We can find the optimal solution to this problem in polynomial time using linear programming.

\paragraph{Linear programming}
Suppose you're a farmer with an $L\ km^2$ field and an amount $F\ kg$ of fertilizer. You want to maximize your profit. We call:
\begin{align*}
&S_p = \text{selling price of potatoes per square kilometer}\\
&S_b = \text{selling price of barley per square kilometer}\\
&F_p = \text{kg of fertilizer per square kilometer of potatoes}\\
&F_b = \text{kg of fertilizer per square kilometer of barley}\\
\end{align*}

You also have a limited amount of fertilizer. You want to maximize the function:

\begin{align*}
	&\text{max: } S_p x_p + S_b x_b\\
	&\begin{cases}
	x_p + x_b \leq L\\
	F_p x_p + F_b x_b \leq F
	\end{cases}
\end{align*}

We can solve this geometrically or analytically.

A general primal LP problem is expressed in the form:

\begin{align*}
	&\bar{x} = \begin{pmatrix}
	x_1\\
	\dots \\
	x_n
	\end{pmatrix}\\
	&\begin{cases}
	\text{max: } \bar{c}^T \bar{x}\\
	A \bar{x} \leq \bar{b}^T\\
	\bar{x} \geq \bar{0}
	\end{cases}\\
	&A \in \mathscr(Q)^{m\times n},\, \bar{b} \in \mathscr(Q)^{m},\, \bar{c} \in \mathscr(Q)^{n}\\
\end{align*}

Its dual is in the form:

\begin{align*}
&\begin{cases}
\text{min: } \bar{b}^T \bar{y}\\
\bar{y}^T A \geq \bar{c}\\
\bar{y} \geq \bar{0}
\end{cases}\\
\end{align*}

The weak duality theorem states that $opt(\text{primal}) \leq opt(\text{dual})$ if they are both feasible. The strong duality states that if the optimal solution exist then $opt(\text{primal}) = opt(\text{dual})$

We can hope to find an algorithm that runs in $O\left(\left(n\cdot m \cdot \log |A|_\infty \right)^c\right)$ which is polynomial time. With $\log |A|_\infty $ being the max number of bits needed to represent each number.

\paragraph{k-Max-Cover LP}
We can express the k-max-cover problem as a LP problem. 

\begin{align*}
&m \text{ sets }\ S_1,\dots,S_m\ \subseteq [n]\\
&\text{max: } \sum y_j\\
&\begin{cases}
\sum_{i=1}^{m} x_i \leq k\\
y_j \leq \sum_{i: j\in S_i}^{} x_i \\
\end{cases}\\
&0 \leq x_i,\, y_j \leq 1
\end{align*}

We introduce one variable $x_i$ for each set $S_i$ and one variable $y_j$ for each $j \in [n]$

\newtheorem{linearmaxcover}{LP max-cover}

\begin{linearmaxcover}
	We state that:
	\[
	\left(1-\frac{1}{e}\right) opt_{LP} \leq opt_{ML} \leq opt_{LP}
	\]
\end{linearmaxcover}

\begin{proof}
	Ipse dixit
\end{proof}

\paragraph{Densest Subgraph LP}
\begin{align*}
	&G=(V,E)\\
	&\text{max: } f(S) = \frac{|E(S)|}{|S|}\\
	&S\subseteq V,\, E(S) = \{ \{i, j\} | \{i, j\} \in E, \{i, j\} \subseteq S \}\\
	&\begin{cases}
	\text{max: } \sum_{\{i, j\} \in E}^{} x_{\{i, j\}}\\
	x_{\{i, j\}} \leq y_i\ \text{i pick one edge only once}\\
	x_{\{i, j\}} \leq y_j\ \text{i pick one edge only once}\\
	\sum_{i = 1}^{n} y_i \leq 1\\
	x_{\{i, j\}}, y_i \geq 0\\
	\end{cases}
\end{align*}

\paragraph{Lemma 1} $\forall S\subseteq V,\, opt_{LP} \geq f(S)$
\begin{proof}
	\begin{align*}
		&y_i = \frac{1}{|S|},\, i \in S\\
		&y_i = 0,\, i \notin S\\
		&x_{\{i, j\}} = \frac{1}{|S|},\, \{i, j\} \subseteq S\\
		&x_{\{i, j\}} = 0,\, \{i, j\} \nsubseteq S\\
	\end{align*}
	
	\begin{align*}
		&\sum y_i = |S|\frac{1}{|S|} = 1\\
	\end{align*}
	
	\begin{align*}
		&\{i, j\} \in E\\
		&-\ \{i, j\} \nsubseteq S\\
		&\begin{cases}
		x_{\{i, j\}} \leq y_i\\
		\{i, j\} \leq y_j
		\end{cases} \iff \overbrace{{\{i, j\}}}^{=0} \leq \overbrace{min(y_i, y_j)}^{=0}\\
	&-\ \{i, j\} \subseteq S\\
	&\frac{1}{|S|} = x_{\{i, j\}} \leq min(y_i, y_j) = \frac{1}{|S|}\\
	\end{align*}
	
	So our problem is feasible
	
	\begin{align*}
	&\sum x_{\{i, j\}} = |E(S)| \frac{1}{|S|} = f(S)\\
	\end{align*}
\end{proof}

	Now we want to prove the other verse of the bound:
	
	\paragraph{Lemma 2}
	\begin{align*}
	&\text{Let } \{x_{\{i, j\}}, y_i\}\text{ be an optimal solution to our LP, with value } v\\
	&\text{we can claim that:}\\ 
	& \exists S \subseteq V,\, f(S) \geq v\\
	\end{align*}
	
	We assume that:
	\[
	\forall \{i, j\} \in E,\ x_{\{i, j\}} = min(y_i, y_j)
	\]
	\begin{proof}

	By contradiction assume that:
	\[
	\exists \{i, j\} \in E,\ x_{\{i, j\}} < min(y_i, y_j)
	\]
	Then it means our algorithm is not finished since we could have taken another step to further increase the current value of our solution.
		
	\end{proof}

	\begin{align*}
	&\text{Let } S(r) = \{i\ |\ y_i \geq r\}\\
	&E(r) = \{\{i, j\}\ |\ x_{\{i, j\}} \geq r \}\\
	\end{align*}
	
	We claim that: 
	\begin{align*}
	&\forall O \leq r \leq max\{y_i\}\\
	&\forall \{i, j\} \in E,\ \{i, j\} \in E(r) \iff \{i, j \} \subseteq S(r)\\
	\end{align*}
	
	\begin{align*}
	&\{i, j\} \in E(r),\ \implies  x_{\{i, j\}} \geq r\\
	&min(y_i, y_j) =  x_{\{i, j\}} \geq r\\
	&y_i \geq r\ \wedge\ y_j \geq r\\
	& i,j \in S(r)
	\end{align*}
	
	\begin{align*}
	&\{i, j\} \subseteq S(r)\\
	&r\leq  x_{\{i, j\}} = min(y_i, y_j)\\
	&\{i, j\} \in E(r)\\
	\end{align*}
	
	\begin{align*}
	\int_{0}^{max\{y_i\}}|S(r)|\ dr &= \sum_{i=1}^{n} \int_{0}^{max\{y_i\}} [r\leq y_i]\ dr\\
								  &= \sum_{i=1}^{n} \int_{0}^{y_i} 1\ dr\\
								  &= \sum_{i=1}^{n} y_i\\
	\end{align*}
	
	\begin{align*}
	\int_{0}^{max\{y_i\}} |E(r)|\ dr &= \sum_{\{i, j\} \in E} \int_{0}^{max\{y_i\}} [r\leq x_{\{i, j\}}]\ dr\\
								   &= \sum_{i=1}^{n} \int_{0}^{x_{\{i, j\}}} 1\ dr\\
								   &= \sum_{i=1}^{n} x_{\{i, j\}}\\
	\end{align*}
	
	So:
	
	\begin{align*}
	&\int_{0}^{max\{y_i\}}|S(r)|\ dr = \sum_{i=1}^{n} y_i\\
	&\int_{0}^{max\{y_i\}} |E(r)|\ dr = \sum_{i=1}^{n} x_{\{i, j\}}\\
	\end{align*}
	
	My final claim is:
	\[
	\exists r \in [0, max\{y_i\}]\quad |E(r)| \geq v\cdot|S(r)|
	\]
	
	\begin{proof}
		

	\begin{align*}
	&\text{By contradiction }\forall r \in [0, max\{y_i\}]\ |E(r)| < v\cdot|S(r)|\\
	&\int_{0}^{max\{y_i\}} |E(r)|\ dr < \int_{0}^{max\{y_i\}}|S(r)|\ dr = v \underbrace{\sum_{i=1}^{n} y_i}_{=1} = v\\
	& \text{But the above equation is also equal to: } \sum_{\{i, j\} \in E} x_{\{i, j\}} = v \Longrightarrow\!\Longleftarrow
	\end{align*}
	\end{proof}

\section{26-10-17}

\paragraph{Densest subgraph problem} Finding the densest subgraph of $G=(V,E)$ is defined as finding that set of nodes $S$ such that 

\[
\delta (S) = \frac{|E(S)|}{|S|} \text{ is maximized}
\]

\begin{algorithm}[H]
	\caption{Greedy-Subgraph( graph: G)}
	\label{alg:sub-greedy}
	\begin{algorithmic}
		\STATE $S_0 \leftarrow V$
		\FOR{$i=0$ to $|V-1|$}
		\STATE let $w_i \in S_i$ be a node of min degree in $G\ |\ S_i$
		\STATE $S_{i+1} \leftarrow S_i - \{w_i\}$
		\ENDFOR
		\RETURN the $S_i$ that maximizes $\delta(S_i)$
	\end{algorithmic}
\end{algorithm}

\newtheorem{subgraphbound}{Greedy subgraph is a 2 approx to DSG}

\begin{subgraphbound}
The solution returned by greedy-subgraph is a 2-approximation to the DSG.
\end{subgraphbound}

\begin{proof}
	\begin{align*}
	&G=(V,E) \rightarrow D(V,A) \text{orientation of G}\\
	&\forall \{v, w\} \in E \implies |\{(v, w), (w, v)\}\cap A| = 1\\
	&\forall (v, w) \in A \implies \{v, w\}\in E\\
	\end{align*}
	
	Claim: If $D(V,A)$ is an orientation of $G=(V,E)$ and $d_M$ is the maximum in-degree of $D(V,A)$ then $\forall S \subseteq V: \delta(S) \leq d_M$
	
	\begin{align*}
		&\delta (S) = \frac{|E(S)|}{|S|}\\
		&|E(S)| \leq \sum_{v\in S}^{} indeg_D(v) \leq \sum_{v\in S}^{} d_M = |S| d_M\\
		&\delta (S) = \frac{|E(S)|}{|S|} \leq d_M\\
	\end{align*}
	
	We build the orientation as the algorithm proceeds. When we remove a node we orient all the edges that were connected to that node towards it. 
	
	Let $D(V,A)$ be the resulting orientation of $G(V,E)$
	\begin{align*}
	&\forall i=0,\dots, |V|-1\\
	&{indeg}_D(w_i) = {deg}_{S_i}(w_i) = {min}_{v\in S_i} deg_{S_i}(v) \leq {avg}_{v \in S_i} deg_{S_i}(v)\\
	&= \frac{\sum_{v \in S_i}^{} deg_{S_i}(v)}{|S_i|} =  \frac{2|E(S_i)|}{|S_i|} = 2\delta(S_i)
	\end{align*}
	
	Let $ i^* $  be and index maximizing $ indeg(w_i^*) $\\
	If $ S^* $ is the optimal solution then:
	\begin{align*}
	&\delta(S^*) \leq indeg(w_i^*) \leq 2\delta(S_{i^*}) \\
	&\delta(S_{i^*}) \geq \frac{1}{2} \delta(S^*) 
	\end{align*}

\end{proof}

\begin{algorithm}[H]
	\caption{Parallel-Greedy-Subgraph( graph: G)}
	\label{alg:par-sub-greedy}
	\begin{algorithmic}
		\STATE $S_0 \leftarrow V$
		\FOR{$i=0$ to $|V-1|$}
		\STATE let $w_i \in S_i$ be a node of having at most $ (1+\epsilon^+) $avg degree in $G\ |\ S_i$
		\STATE $S_{i+1} \leftarrow S_i - \{w_i\}$
		\ENDFOR
		\RETURN the $S_i$ that maximizes $\delta(S_i)$
	\end{algorithmic}
\end{algorithm}

This algorithm can be parallelized
\begin{algorithm}[H]
	\caption{Quick-Greedy-Subgraph( graph: G)}
	\label{alg:quick-sub-greedy}
	\begin{algorithmic}
		\STATE $S_0 \leftarrow V$
		\STATE $ i\leftarrow 0 $
		\WHILE{ $ S_i \neq \emptyset$}
		\STATE $ A(S_i) \leftarrow \{v|v\in S_i \wedge deg_{S_i}(v) \leq (1+\epsilon)avg_{w\in S_i} deg_{S_i}(w) \} $
		\STATE $ S_{i+1} \leftarrow S_i - A(S_i) $
		\STATE $ i \leftarrow i+1 $
		\ENDWHILE
		\RETURN the $S_i$ that maximizes $\delta(S_i)$
	\end{algorithmic}
\end{algorithm}

\newtheorem{quickgreedy}{Quick Greedy Bound}

\begin{quickgreedy}
	The quick greedy algorithm will return a $ (2+2\epsilon)-approx $ and it will run for $ O\left(\frac{1}{2}\log n\right) $ iterations
\end{quickgreedy}

\begin{proof}
	Let $ S^* $ be an optimal solution. We claim that $ \forall v \in S^*,\ deg_{S^*}(v) \geq \delta(S^*) $. This means that we remove a node with degree less than the average we increase te average density.\\
	
	By contradiction:
	
	\begin{align*}
		&\exists v \in S^*,\ deg_{S^*}(v) < \delta(S^*)\\
		&\delta(S^* - \{v\}) = \frac{|E(S^* - \{v\})|}{|S^*| -1} =  \frac{|E(S^*)| - deg_{S^*}(v)}{|S^*| -1} > \frac{|E(S^*)| - \delta(S^*)}{|S^*| -1}=\\
		&=\frac{|E(S^*)|}{|S^*|} \frac{|S^*|}{|S^*| -1} - \frac{\delta(S^*)}{|S^*| -1}=\\
		&\delta(S^*)\left(\frac{|S^*|}{|S^*| -1} - \frac{1}{|S^*| -1}\right) = \delta(S^*)\quad \rightarrow\!\leftarrow
	\end{align*}
\end{proof}

	Claim: $ |A(S_i)| \geq 1 $ if $ |S_i| \geq 1$
	\begin{proof}

	Let $ i^* $ be the first integer s.t. quick-greedy removes some element of the optimal solution $ S^* $ from $ S_{i^*} $ ( to get $ S_{i^* + 1} $)
	
	\begin{align*}
	&|A(s_{i^*)} \cap S^* | \geq 1 \quad \left(\forall j < i^* |A(S_j) \cap S^*| = 0 \right)\\
	&S_{i^*} \supseteq S^*
 	\end{align*}
 	
 	Let $ v $ be a node in $ A(S_i^*) - S^* $:\\
 	\begin{align*}
 		&\delta(S^*) \leq deg_{S^*}(v) \leq deg_{S_{i^*}}(v) \leq (1+\epsilon) avg_{w \in S_{i^*}} deg_{S_{i^*}}(w)=\\
 		&=(1+\epsilon) \frac{2|E(S_{i^*})|}{|S_{i^*}|} = (1+\epsilon) 2 \delta(S_{i^*})\\
 		& 2|E(S_{i^*}| = \sum_{v \in S_i}^{} deg_{S_i}(v) =\\
 		&\sum_{v \in A(S_i)}^{} deg_{S_i}(v) + \sum_{v \in S_i - A(S_i)}^{} deg_{S_i}(v)\geq\\
 		&\geq \sum_{v \in S_i - A(S_i)}^{} deg_{S_i}(v) > \sum_{v \in S_i - A(S_i)}^{} (1+\epsilon)\ avg_{w \in S_i}deg_{S_i}(v)\\
 		&=(1+\epsilon) |S_i - A(S_i)|  avg_{w \in S_i}deg_{S_i}(w) = |S_i - A(S_i)| \frac{2|E(S_{i})|}{|S_{i}|} (1+\epsilon)\\
 		&2|E(S_i)| > (1+\epsilon) |S_i - A(S_i)| \frac{2|E(S_{i})|}{|S_{i}|}\\
 		& \frac{1}{1+\epsilon} > \frac{|S_i| - |A(S_i)|}{|S_i|} = 1 - \frac{|A(S_i)|}{|S_i|}\\
 		&\frac{|A(S_i)|}{|S_i|} > 1 - \frac{1}{1+\epsilon} = \frac{1+\epsilon}{1+\epsilon} - \frac{1}{1+\epsilon} = \frac{\epsilon}{1+\epsilon} 
 	\end{align*}
 	
 	We now bound the number of operations:
 	\begin{align*}
 		&x\leq \frac{1}{2}\\
 		&\log(1+x) = \Theta(x)\\
 		& O\left(\log_{1+\frac{\epsilon}{1+\epsilon}} |V|\right) = \\
 		& O\left(\frac{\log |V|}{\log \left( 1+\frac{\epsilon}{1+\epsilon} \right)} \right) = O\left(\frac{\log |V|}{\frac{\epsilon}{1+\epsilon}} \right) =  O\left(\frac{\log |V|}{\epsilon} \right)
 	\end{align*}
\end{proof}
\section{30-10-17}

Consider a space in which you have some points. You'd like to learn something about these points, such as determining clusters.

Suppose we have a metric in our space. That is a function that respects:

\begin{align*}
& d: V \times V \rightarrow \mathbb{R},\ \forall x,y,z, \in V\\
&1)\ d(x,y) \geq 0\\
&2)\ d(x,y) = d(y,x)\\
&3)\ d(x,x) = 0\\
&4)\ d(x,y) \leq d(x,z) + d(z,y)\\
\end{align*}

The metrics we are used to are normal metrics:

\begin{align*}
&V = \mathbb{R}^D\\
&d(\bar{x}, \bar{y}) = \sqrt{\sum_{i=1}^{D} \left(x_i - y_i\right)^2}\\
\end{align*}

But there are also LP metrics:

\begin{align*}
&V = \mathbb{R}^D\ p \geq 1\\
&d(\bar{x}, \bar{y}) = {\left(\sum_{i=1}^{D} |x_i - y_i|^p\right)}^{\frac{1}{p}}\\
\end{align*}


Does there exist a subset of V ok cardinality k such that one can assign each $v \in V$ to some point $w(v)\in S$ so that $ d(v, w(v)) \leq r $. That is finding k-clusters of max radius r.

Question: what is the maximum $ r $ that admits a k-clustering of $ (V,d) $.

Let's overload the distance function to accept a set of points as second argument. We can define the distance between a point and the set as:
\begin{align*}
& d: V \times V \rightarrow \mathbb{R}\\
&d(v,w)\ v, w \in V\\
&S \subseteq V\\
&d(v, S) = {min}\_{w \in S} d(v, w)\\
\end{align*}

Now we can present a greedy k-center algorithm. The algorithm start by picking a point at random. We then put 
\begin{algorithm}[H]
	\caption{Greedy-K-center( metric: (V,d), integer: k)}
	\label{alg:k-center}
	\begin{algorithmic}
		\STATE $S \leftarrow \{x\}$ chosen at random, $ x \in V $
		\WHILE{$ |S| < k  $}
		\STATE let $ x \in V\setminus S $ be a point maximizing $ d(x, S) $
		\STATE $ S \leftarrow S \cup \{x\} $
		\ENDWHILE
		\RETURN $ S $
	\end{algorithmic}
\end{algorithm}

\newtheorem{kgreedybound}{K-center Greedy Bound}
\begin{kgreedybound}
	The Greedy K-center algorithm produces a 2 approximation
\end{kgreedybound}

\begin{proof}
	Let $ S^* \subseteq V $ be an optimal solution $ \left(|S^* = k|\right)  $, having radius $ r^* $. Assign each $ y \in V $ to the cluster of one of the closest $ r^* \in S^* $. Let $ {\left\{ c \left( r_i^* \right) \right\}}_{r_i^* \in S^* }  $ be the resulting clustering.
	Let $ S $ be the final greedy solution $ \left(|S = k|\right)  $.
	
	\paragraph{Lemma} $ \forall x \in S^*,\ \forall y,z \in C(x),\ d(y,z) \leq 2r^* $\\
	$ d(y,z) \leq d(y,z) + d(z,x) \leq r^* + r^* = 2r^* $\\
	
	Suppose that $ \forall x \in S^*,\ C(x) \cap S \neq \emptyset $.\\
	Pick $ y \in V $, and say that $ y\in C(x), \text{ for } x \in S^* $ this means that: $ \exists z \in C(x) \cap S $ such that $ d(y, S) \leq d(y, z) \leq 2r^*$ because z is inside S ($ z \in S $) and because of the Lemma.
	
	\paragraph{Voronoi diagrams} The $ C(x) \forall x \in V $ produce a neat partition of the space called a Voronoi diagram. Regarding our proof. Suppose that\\
	$ \exists x \in S^*,\ C(x) \cap S = \emptyset $. By the pigeon hole principle $ \exists x' \in S^*\ |C(x') \cap S| \geq 2 $. Let $ y, y' \in C(x') \cap S,\ y\neq y' $. Suppose that $ y $ was added to S by k-greedy before $ y' $. Let $ S' $ be the set of points that k-greedy selected before $ y' $. That is $ S' = \underbrace{x_1, x_2, \dots , y, \dots}, y', \dots , x_k  $. Pick $ z \in V $. 
	\begin{align*}
	d(z, S) &\leq d(z, S')\ \text{Since } (S'\subseteq S)\\
	&\leq d(y', S')\ \text{For how greedy picks points}\\
	&\leq d(y', y)\ \text{Since } (y \in S')\\
	&\leq 2r^*\ \text{By the Lemma }\\	
	\end{align*}
\end{proof}

\newtheorem{kcenterhardness}{K-Center is NP hard}
\begin{kcenterhardness}
	Providing a $ (2 - \epsilon) $-approx to "k-center" is NP-hard.
\end{kcenterhardness}
\begin{proof}
	By reduction to the dominating set problem. Which takes as an input $ G=(V,E),\ k $ and asks: does there exists a $ S \subseteq V,\ |V| = k$ such that $ \forall w \in V $ either $ w \in S $ or $ w $ has a neighbour in $ S $.\\\\
	Consider a "K-center" instance of z.\\
	We have k points of the metric.\\
	Will ask for a radius $ r^* = 1 $ covering\\
	$ d(v,w) = \begin{cases}
	1&\left\{v,w\right\} \in E\\
	2&\text{otherwise}\\
	\end{cases}  $\\\\
	Def: A $ (1,2) $-metric is a a $ (V,d) $ such that $ d(v,w) \in \left\{ 1,2 \right\} \forall \left\{v,w\right\} \in {v \choose 2}$ this implies that $ \underbrace{d(v,x)}_{1} + \underbrace{d(w,v)}_{1} \geq \underbrace{d(v,w)}_{2} $ This somewhy concludes the proof ???\\\\
	Suppose that I you i only care about the euclidean spaces. And I relax the constraints on the initial point by letting you choose any point in the space as center for a cluster. This problem is harder than the previous one and our algorithm provides a worst approximation in this case. 
\end{proof}

\section{2-11-17}
\paragraph{1-Centre problem} Given a metric $ (V,d) $ and a subset of point $ W \subseteq V $ find a k-center of radius 1. Generally we have.
\begin{align*}
&min\ &max\ &min\ &d(w, c) \leq \delta\\
&c \in {V \choose k} &w \in W &c\in C
\end{align*}

So for the 1-centre problem. $ C^* = \{c^*\} \subseteq V,\ c^* \in V $\\
\paragraph{Lemma} 

\begin{align*}
&max_{w \in W} d(w, c^*) &\geq max_{\{w_1w_2\} \in {W \choose 2}} \frac{d(w_1, w_2)}{2}\\
&\frac{1}{2}(\underbrace{d(w_1, c^*)}_{\underbrace{\frac{1}{2}(max_{w \in W} d(w, c^*) + max_{w \in W} d(w, c^*))}_{max_{w \in W} d(w, c^*)}} + d(c^*, w_2) )&\geq d(w_1, w_2)\\
\end{align*}

Pick $ w in W $
\begin{align*}
&max_{w' \in W} d(w, w') \leq max_{\{w_1w_2\} \in {W \choose 2}} d(w_1, w_2) \leq 2max_{w' \in W} d(w', c^*)\\
\end{align*}

\paragraph{K-Median} $  min_{c \in {W \choose k}}  \sum\limits_{v \in V} min_{c \in C} d(v, c) \leq \frac{\delta}{|V|}$. K-median admits a $ 3+\epsilon $ approx. in time $n^{O(\frac{1}{2})}$

\paragraph{1-Median} Consider the case where k = 1.
\[
\sum\limits_{w\in W} min_{c\in \{C^*\}} d(w, c) = \sum\limits_{w\in W} d(w, c^*) = Opt\\
\]

If $ w $ is chosen UAR from $ W $ then $ \mathbb{E}[\text{1-median cost of w}] \leq 2Opt $. 

\begin{align*}
\mathbb{E}[\text{1-median cost of w}] &= \frac{1}{|W|} \sum\limits_{w\in W} \sum\limits_{w' \in W} d(w, w') = \frac{2}{|W|} \sum\limits_{\{a,b\} \in {W\choose 2}} d(a,b)\\
&\leq \frac{2}{|W|} \sum\limits_{\{a,b\} \in {W\choose 2}} \left(d(a,c^*) + d(c^*, b)\right)\\
&= \frac{2}{|W|}(|W|-1) \underbrace{\sum\limits_{x \in W} (d(x,c^*)}_{} = \frac{2}{|W|}(|W| -1) Opt &\\
&= \left(2 - \frac{2}{|W|}\right) Opt\\
\end{align*}
Is this bound tight? Apparently yes:
\begin{align*}
&\sum\limits_{w' \in W} (d(w',c^*) = |W|\\
&\sum\limits_{w' \in W} (d(w, w') = 2(|W|-1)\\
&\frac{2|W|-2}{|W|} = 2 - \frac{2}{|W|}
\end{align*}

This problem is polynomial in V if we consider the K-centre where we can only pick centres from our set of nodes. When we relax the constraint and say we can pick any point from our metric becomes NP-hard.\\
We will prove that by reduction to 3-SAT. Consider n boolean variables and a formula
\[ \zeta \subseteq {\{x_1, \dots , x_n, \bar{x_1}, \dots, \bar{x_n} \} \choose 3}\]
Does there exists and assignment to the boolean variables that guarantees that $ \forall c \in \zeta $ at least one literal of c is true. We add as many nodes as the possible assignments to the clauses of our formula.

\begin{proof}
	\begin{align*}
	&\zeta = \{c_1, \dots c_n\}&d(c_i, c_j) = 2\ \forall \{c_i,c_j\} \in {\zeta\choose 2}\\
	&d(c_i, A_j) = \begin{cases}
	1&\text{if }A_j\ SAT\ C_j\\
	2&\text{otherwise} 
	\end{cases}\\
	&d(A_i, A_j) = 2 &\forall i, j
	\end{align*}
	
	Suppose the formula is satisfiable. What is the best centre? \\
	If the formula is satisfiable we pick the $ A_i $ that satisfies our formula and the 1-centre cost $ \leq m $ with m the number of nodes.\\
	If the formula is not satisfiable then 1-centre cost $ \geq 2(m-1)$
\end{proof}

\paragraph{Correlation clustering}
Consider a set $ V = \{w_1, \dots, v_n\} $ and a function $ f:V\times V \rightarrow \{1, -1\} $. The function spits out 1 if the nodes are similar, -1 otherwise. We want to partition V into clusters so to minimize the number of pairs of similar elements that are divided by the clustering plus the number of dissimilar elements that are not divided. That is we pay 1 if we divide two similar elements, and 1 if we put together two dissimilar elements. The unbounded version of this problem is easy. Just put every node in a different cluster.\\
If the optimal solution has cost 0 we can look at the graph induced by the positive edges and return the connected components clusters.\\
We use the function f to assign labels to the edges of the graph.
\paragraph{Correlation clustering problem} Find a partition of $ V $ of minimum cost. We consider a case of the problem where each pair of nodes is comparable so the union of the positive and negative edges $ E^+ \cup E^- = {V \choose 2} $. Clearly the problem is NP-hard but we can show that it $ \exists $ a 3-approx to the CC-problem. Since the objective function for this problem is maximized at 0 we must show that if the optimal solution is 0 then the clustering can be computed in polynomial time. This is done by finding the connected components of the graph.


\begin{algorithm}[H]
	\caption{Greedy-Clustering(graph: G)}
	\label{alg:g-clustering}
	\begin{algorithmic}
		\STATE $C \leftarrow \emptyset $
		\WHILE{$ V \neq \emptyset  $}
		\STATE pick $ v \in V $ U.A.R.
		\STATE$ C \leftarrow \{v\} \cup \{w\ |\ w \in V \wedge \{v, w\}\in E^+ \}$
		\STATE $C\ \cup\! = \{c\} $
		\STATE $V \leftarrow V \setminus C  $
		\ENDWHILE
		\RETURN $ C $
	\end{algorithmic}
\end{algorithm}

\newtheorem{clusterbound}{Greedy cluster bound}
\begin{clusterbound}
	$ \mathbb{E}[\text{cost of greedyCC}] \leq 3opt $
\end{clusterbound}

There are some cluster on which greedy cc performs really bad. Consider for example a star topology graph where all the edges to the star centre are positive and the rest are negative. Suppose we pick the star centre our cost becomes $ c = {(n-1) \choose 2} $. However we could've picked a point of the star and we would've had a cost of $ n-2 $. However we're still safe since the bad event happens with a very tiny probability if the star is large.

\begin{proof}
	Let Cost$^-_i $ = \# of negative edges inside the $ i $th cluster and\\
	Cost$^+_i $ = \# of positive edges from cluster $ i $ to cluster $ j $ where $ j \geq i+1 $.\\ Finally we define Cost$_i $ = Cost$^+_i $ + Cost$^-_i $.
	\paragraph{Lemma} $ \sum  $  Cost$_i $ = Cost.\\
	\paragraph{Def} A bad triangle is a triple $ \{v_1, v_2, v_3 \} \in {V \choose 3}$. S.t. $ \mid {B \choose 2} \cap E^- \mid\ = 1$.
	\paragraph{Lemma} Cost$_i $ = $ \mid\{ B \mid $  $ B $ is a bad triangle containing the $ i $th pivot and which are completely contained in the remaining graph, when the $ i $th pivot is chosen $ \}\mid $.  Consider again the case of the star. Every positive edge that goes from a leaf to outside the cluster would've been picked it there wasn't a negative edge going from the pivot to that node. So that's a bad triangle centred in the leaf. However as we showed before the probability of picking the star centre is very low so we can kill many bad triangles at once.
	\paragraph{Corollary} The $ \mathbb{E} [\text{cost of greedycc}] = \mathbb{E}[$\# of bad triangles hit by a pivot when they are still completely in the graph$] = \sum\limits_{T \in \beta}\Pr\{T \text{is Hit}\} = 3\sum\limits_{T \in \beta} q_T$. A bad triangle is completely in the graph if it's still in the graph when we removed some nodes for an iteration.\\
	If $ \{a, b, c\} $ is a bad triangle, let $ T_{ab,c} $ be the event that $ c $ is chosen as pivot while $ \{a, b, c\} $ are still all part of the graph. So if \[
	p_{ab,c} = \Pr\{T_{ab,c}\}\; p_{ab,c}=p_{ac,b}=p_{bc,a} \equiv q_{abc}\ \Pr\{\{abc\} \text{is Hit}\} = 3q_{abc}\]. We will now provide a linear relaxation of CC.
	\[
	LP = 
	\begin{cases}
	min \sum_{\{v, v'\}\in {V\choose 2}}^{} x_{\{v, v'\}}\\
	x_{\{v, v'\}} + x_{\{v', v''\}} + x_{\{v, v''\}} \geq 1 \forall \{v, v', v''\}\in \beta \\
	0 \leq x_{\{v, v'\}}	
	\end{cases}
	\]
	
	Let us now think of the optimal solution. LP$ ^* \leq $OPT
	\[	x_{\{v, v'\}} = 
	\begin{cases}
	1&\text{if } \{v, v'\} \text{has cost of 1 in OPT}\\
	0&\text{otherwise}
	\end{cases}
	\]
	
	Recall that if we have LP problem then we got that DUAL$ ^* \leq$ LP$ ^* $ if LP is a minimization problem.
	\[
	LP = 
		\begin{cases}
		min \sum_{\{v, v'\}\in {V\choose 2}}^{} x_{\{v, v'\}}\\
		x_{\{v, v'\}} + x_{\{v', v''\}} + x_{\{v, v''\}} \geq 1 \forall \{v, v', v''\}\in \beta \\
		0 \leq x_{\{v, v'\}}	
	\end{cases}
	\]
	
	We had that a general primal LP problem is expressed in the form:
	
	\begin{align*}
	&\bar{x} = \begin{pmatrix}
	x_1\\
	\dots \\
	x_n
	\end{pmatrix}\\
	&\begin{cases}
	\text{max: } \bar{c}^T \bar{x}\\
	A \bar{x} \leq \bar{b}^T\\
	\bar{x} \geq \bar{0}
	\end{cases}\\
	&A \in \mathscr(Q)^{m\times n},\, \bar{b} \in \mathscr(Q)^{m},\, \bar{c} \in \mathscr(Q)^{n}\\
	\end{align*}
	
	Its dual is in the form:
	
	\begin{align*}
	&\begin{cases}
	\text{min: } \bar{b}^T \bar{y}\\
	\bar{y}^T A \geq \bar{c}\\
	\bar{y} \geq \bar{0}
	\end{cases}\\
	\end{align*}
	
	So in our case we have
	
	\[
	DUAL = 
	\begin{cases}
	max\ \sum\limits_{T\in \beta}y_T\\
	\sum\limits_{\substack{T\in \beta\\T\supset \{v, v'\}}}y_T \leq 1\ &\forall \{v, v'\} \in {V \choose 2}\\
	Y_T \geq 0 &\forall T\in\beta\\
	\end{cases}
	\]
	
	\begin{align*}
	&\sum\limits_{\substack{T\in \beta\\T\supset \{v, v'\}}}q_T = \sum\limits_{\substack{v''\\\{v, v', v''\} \in \beta}} q_{\{v, v', v''\}} = \sum\limits_{\substack{v''\\\{v, v', v''\} \in \beta}} p_{vv', v''} = \sum\limits_{\substack{v''\\\{v, v', v''\} \in \beta}} \Pr\{T_{vv', v''}\} = 1
	\end{align*}
	
	Since these events are all disjoints then the sum of their probabilities is 1. So we proved that
	\[
	\sum q_T \leq \text{DUAL}^* \leq \text{LP}^* \leq \text{OPT}
	\] 
	And thus $ \mathbb{E}[\text{greedycc}] \leq 3q_T $
	
\end{proof}

\section{Markov Chains}
A Markov chain is a sequence $ X_0, \dots,X_n $ of random variables on $ S $ generic state space, which satisfies:
\begin{enumerate}
	\item Memorylessness :\[ \Pr\{X_t = x \mid X_{t-1} = y,\dots, X_{0} = y_0\} = \Pr\{X_t = X \mid X_{t-1} = y \}\]
	E.g. : the drunkard walk\\
	Consider a drunkard wandering around 2 bars and their home. Let's say that they can be found at a finite set of points $ \mathbb{S} = \{0, 1, -1, 2, -2\} $. We give the distribution of probability for their position
	\[
	X_0 = \begin{cases}
	0&pr = 1\\
	1&pr = 0\\
	2&pr = 0\\
	-1&pr = 0\\
	-2&pr = 0\\
	\end{cases}
	\]
	
	\[
	\forall x,y \in \mathbb{S}\ \Pr\{X_t =x \mid X_{t-1} = y\} = \Pr_{xy}
	\]
	
	For each possible transition I can have a different probability. This is given in a transition matrix $ P $.
	
	\[
	P=
	\left(
	\begin{matrix}
	0&1&0&0&0\\
	\frac{1}{2}&0&\frac{1}{2}&0&0\\
	0&\frac{1}{2}&0&0&0\\
	0&\frac{1}{2}&0&\frac{1}{2}&0\\
	0&0&0&1&0\\
	\end{matrix}\right)
	\]
	
	Now if i have the position $ X_0 $ given by putting a 1 in the probability of the position at which the drunkard is, I have:
	$ \mathscr{F}_0 = (0, 0, 1, 0, 0) $. The probability vector at time $ t $ is then:
	\begin{align}
	&\mathscr{F}_t = \mathscr{F}_0 P^T
	\end{align}
	\item Hitting time: we call the hitting time the expected value of the time it takes the drunkard to reach a certain point, denoted by $ \mathscr{H}_x\left(T_{\Omega\subseteq \mathbb{S}}\right) $, that is the time it takes the drunkard to go from point $ X $ to a certain subset $ \Omega \subseteq \mathbb{S} $. Let's say the drunkard starts from point 0 and I want to know
	\begin{align*}
	\mathscr{H}_0 \left(T_{\{-2, 2\}}\right) &= \frac{1}{2}\left[\mathscr{H}_0 \left(T_{\{-2, 2\}}\right) + 1 \right] + \frac{1}{2}\left[\mathscr{H}_{-1} \left(T_{\{-2, 2\}}\right) + 1 \right]\\
	\mathscr{H}_1 \left(T_{\{-2, 2\}}\right) &= \frac{1}{2}\left[\mathscr{H}_0 \left(T_{\{-2, 2\}}\right) \right] + \frac{1}{2}\left[\underbrace{\mathscr{H}_{2} \left(T_{\{-2, 2\}}\right)}_{0} + 1 \right]\\
	\mathscr{H}_{-1} \left(T_{\{-2, 2\}}\right) &= \frac{1}{2}\left[\mathscr{H}_0 \left(T_{\{-2, 2\}}\right) + 1 \right] + \frac{1}{2}\\
	\mathscr{H}_{0} \left(T_{\{-2, 2\}}\right) &= 4
	\end{align*}
	\item Return time: the expected time for a drunkard to return to the original point from where they started.
	\item Convergence: if we start from any $ \mathscr{F}_0 $ can it be that we find \[
	\mathscr{F}_0 P^1 \rightarrow \mathscr{F}_0 P^2 \rightarrow \dots \rightarrow \mathscr{F}_0 P^t \rightarrow \mathscr{K}
	\]
	such that $ \mathscr{K}P  = \mathscr{K} $  ?. This $ \mathscr{K} $ does not exists, unless we put two other constraints on the chain:
	\begin{itemize}
		\item irreducible $ \rightarrow $ connectivity : $ \forall x,y\ \exists\ t\ \Pr\{X_t = y \mid X_0 - X\} > 0 $
		\item aperiodicity  $ \rightarrow gcd $ returnTimes = 1
	\end{itemize}
	Such a chain is called and Ergodic Markov Chain.
\end{enumerate}

\newtheorem{markov}{Ergodic Markov Chain}
\begin{markov}
	A Markov Chain is Ergodic $ \iff $ there is a unique stationary distribution
\end{markov}
\begin{proof}
	Omitted
\end{proof}

\paragraph{Convergent Markov Chain} If I define \[\pi(X) = \frac{1}{\underbrace{\mathbb{E}_X\left(T_x^t\right)}_{\parbox{7em}{Time to start from X and return to t}}} \text{ then } \pi P = \pi \]
\begin{proof}
Fix some $ z \in \mathbb{S} $.
\begin{align*}
\tilde{\pi} &= \mathbb{E}\left[\text{number of times I visit } X \text{ before returning to } z \right]\\
\tilde{\pi} &= \sum\limits_{t = 0}^{\infty} \Pr_z\{X_t = X, T_z^t > t\}\\
\end{align*}
I want to show that $ \tilde{\pi} P =  \tilde{\pi} $.

\begin{align*}
\sum\limits_{y \in \mathbb{S}} \sum\limits_{t = 0}^{\infty} &\Pr_z\{X_t = X, T_z^t > t\}\Pr\{x,y\}\\
\sum\limits_{t = 0}^{\infty} &\Pr_z\{X_{t+1} = y, T_z^t > t\}\\
\sum\limits_{t = 1}^{\infty} &\Pr_z\{X_{t} = y, T_z^t \geq t\} + \Pr_z\{X_{0} = y, T_z^t \geq 0\} - \sum\limits_{t = 1}^{\infty} \Pr\{X = y, T_z^t \geq y\} = \tilde{\pi}\\
\end{align*}
\end{proof}

Then the return time to x can be defined as $ \sum\limits_{x \in \mathbb{S}} \tilde{\pi}(x) $. And so \[\pi(X) = \frac{\tilde{\pi}(x)}{\sum\limits_{x \in \mathbb{S}} \tilde{\pi}(x)} = \frac{1}{\text{return time to X}}\]


\section{Drunkard walk} Let $ G = (V, E) $ . At each node $ v\in V $ we define.
\[
P = \begin{cases}
\frac{1}{2}&\text{probability to loop}\\
\frac{1}{2deg(v)}&\text{go to one neighbour}
\end{cases}
\]
\newtheorem{randomwalk}{Drunkard Walk Convergence}
\begin{randomwalk}
	For a simple random walk the stationary distribution is: \[
	\pi(v) = \frac{deg(v)}{2|E|}
	\]
\end{randomwalk}
\begin{proof}
	\begin{align*}
	\pi(x) &= \sum\limits_{u \in V} \pi(u)\Pr(u, v)\\
	& = \sum\limits_{u \in N(v)} \frac{deg(u)}{2|E|} \frac{1}{2deg(u)} =\frac{deg(v)}{2|E|}
	\end{align*}
\end{proof}

Now say I have a graph and i want to sample a vertex. Let's define the mixing time: \[d\left(X^t, \pi \right) \leq \frac{1}{4} \] where t is the smallest possible. So this mixing time decreases exponentially since \[ d\left(X^{tc}, \pi \right) \leq \Theta\left(\frac{1}{42^c}\right)\] We also have that:
\[
d(\pi_1, \pi_2) = \frac{1}{2}\sum\limits_{x \in \mathbb{S}}\mid \pi_1(x) - \pi_2(x) \mid
\] that is the total variation distance.

For a simple random walk I know that $ \rightarrow \pi \propto deg(v) $. I want distribution of equal weight to all the vertices, i.e., the uniform distribution.

Sorry future me I have no idea what I'm writing.

\begin{enumerate}
	\item Run the simple random walk
	\item Stop at $ t_{mix} $
	\item Accept  with probability $\frac{1}{deg v} $ or throw it away and run the Markov Chain again
\end{enumerate}

On average the runtime of this algorithm is $ r = d_{avg} t_{mix} $. We modify the random walk probabilities to be:
\[
P = \begin{cases}
1 - \frac{deg(v)}{2d_{max}}&\text{probability to loop}\\
\frac{1}{2d_{max}}&\text{go to one neighbour}
\end{cases}
\]
So the runtime is $ d_{max}t_{mix} $ but the average number of downloads is $ d_{avg}t_{mix} $
\section{TODO}
Oct 5th:  Chain Letter Petitions, Inference\\
Oct 9th: Max-Cover, Submodular functions\\
Oct 12th: Submodular optimization, Submodular Learning\\

\end{document}
